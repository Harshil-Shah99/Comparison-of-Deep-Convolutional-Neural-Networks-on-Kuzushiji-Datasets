{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "VGG16_K49",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "SKQ4bH7qMGrA"
      },
      "source": [
        "# Taking advantage of Colab Pro\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "QMMqmdiYMkvi"
      },
      "source": [
        "## Faster GPUs\n",
        "\n",
        "With Colab Pro you have priority access to our fastest GPUs. For example, you may get a T4 or P100 GPU at times when most users of standard Colab receive a slower K80 GPU. You can see what GPU you've been assigned at any time by executing the following cell."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "23TOba33L4qf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 372
        },
        "outputId": "8e65f241-4e4a-4a13-c93b-a80cd4a9163f"
      },
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
        "  print('and then re-execute this cell.')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mon Aug 10 03:10:16 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 450.57       Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   35C    P0    27W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Sa-IrJS1aRVJ"
      },
      "source": [
        "In order to use a GPU with your notebook, select the Runtime > Change runtime type menu, and then set the hardware accelerator dropdown to GPU."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "65MSuHKqNeBZ"
      },
      "source": [
        "## More memory\n",
        "\n",
        "With Colab Pro you have the option to access high-memory VMs when they are available. To set your notebook preference to use a high-memory runtime, select the Runtime > 'Change runtime type' menu, and then select High-RAM in the Runtime shape dropdown.\n",
        "\n",
        "You can see how much memory you have available at any time by running the following code.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "V1G82GuO-tez",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "aff478bd-8bc1-4586-8b71-b3c0e12cbb55"
      },
      "source": [
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('To enable a high-RAM runtime, select the Runtime > \"Change runtime type\"')\n",
        "  print('menu, and then select High-RAM in the Runtime shape dropdown. Then, ')\n",
        "  print('re-execute this cell.')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Your runtime has 27.4 gigabytes of available RAM\n",
            "\n",
            "You are using a high-RAM runtime!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_hczBOFSaJhd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 161
        },
        "outputId": "6a40ab98-2484-48bf-db83-c1a785f0a523"
      },
      "source": [
        "import numpy as np\n",
        "from sklearn import svm, metrics\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Conv2D, MaxPool2D, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow import keras\n",
        "from keras.utils import to_categorical\n",
        "from keras.optimizers import Adam\n",
        "from keras.preprocessing.image import img_to_array, array_to_img\n",
        "from skimage.transform import resize, rescale\n",
        "import matplotlib.pyplot as plt\n",
        "from skimage import data, color\n",
        "from PIL import Image\n",
        "import cv2\n",
        "from numpy import savez_compressed\n",
        "from google.colab import files\n",
        "from numpy import load\n",
        "from sklearn.model_selection import KFold\n",
        "from keras.callbacks import EarlyStopping\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "filename = '/content/gdrive/My Drive/Research2/np_data/x.npz'\n",
        "x = np.load(filename)['arr_0']\n",
        "\n",
        "filename = '/content/gdrive/My Drive/Research2/np_data/y.npz'\n",
        "y = np.load(filename)['arr_0']\n",
        "\n",
        "print(x.shape)\n",
        "print(y.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n",
            "(270912, 56, 56, 1)\n",
            "(270912, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "BJW8Qi-pPpep"
      },
      "source": [
        "## Longer runtimes\n",
        "\n",
        "All Colab runtimes are reset after some period of time (which is faster if the runtime isn't executing code). While Colab Pro subscribers still have limits, these will be roughly twice the limits for non-subscribers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4PDW1K64acto",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 6081
        },
        "outputId": "478ad742-9814-41ce-c219-8955d5e48aa5"
      },
      "source": [
        "baseAccuracies = []\n",
        "losses = []\n",
        "balancedAccuracies = []\n",
        "f1scores = []\n",
        "precisions = []\n",
        "recalls = []\n",
        "\n",
        "kfold = KFold(n_splits=10, shuffle=True)\n",
        "fold_no = 1\n",
        "callback = EarlyStopping(monitor = 'val_loss', patience = 2, verbose = 1)\n",
        "\n",
        "for train, test in kfold.split(x, y):\n",
        "  print(\"_________________________________________________________________________________________________________________________________\")\n",
        "  print(\"Fold number: \" + str(fold_no))\n",
        "  fold_no = fold_no+1\n",
        "  print()\n",
        "\n",
        "  xtrain = x[train]\n",
        "  xtest = x[test]\n",
        "  \n",
        "  model = tf.keras.applications.VGG16(include_top=True, weights=None, input_shape=(56, 56, 1), classes=49, classifier_activation=\"softmax\",)\n",
        "  model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=Adam(lr=0.0001), metrics=[\"sparse_categorical_accuracy\"])\n",
        "  model.fit(xtrain, y[train], verbose = 1, validation_data = (xtest, y[test]), epochs = 10, callbacks = [callback], batch_size = 32)\n",
        "\n",
        "  test_loss, test_accuracy = model.evaluate(xtest, y[test])\n",
        "  print(\"_________________________________________________________________________________________________________________________________\")\n",
        "  print(\"Test Accuracy:\")\n",
        "  print(test_accuracy)\n",
        "  baseAccuracies.append(test_accuracy)\n",
        "  print(\"_________________________________________________________________________________________________________________________________\")\n",
        "  print(\"Test Loss:\")\n",
        "  print(test_loss)\n",
        "  losses.append(test_loss)\n",
        "  print(\"_________________________________________________________________________________________________________________________________\")\n",
        "  preds = np.argmax(model.predict(xtest), axis=-1)\n",
        "  preds = preds.reshape(len(xtest), 1)\n",
        "  accs = []\n",
        "  for cls in range(49):\n",
        "    mask = (y[test] == cls)\n",
        "    cls_acc = (preds == cls)[mask].mean() \n",
        "    accs.append(cls_acc)\n",
        "  accs = np.mean(accs)\n",
        "  print(\"Final balanced accuracy:\")\n",
        "  print(accs)\n",
        "  balancedAccuracies.append(accs)\n",
        "  print(\"_________________________________________________________________________________________________________________________________\")\n",
        "  f1scores.append(f1_score(y[test], preds, average = 'weighted'))\n",
        "  precisions.append(precision_score(y[test], preds, average = 'weighted'))\n",
        "  recalls.append(recall_score(y[test], preds, average = 'weighted'))\n",
        "  print(\"*********************************************************************************************************************************\")\n",
        "  print()\n",
        "  print()\n",
        "  del model, xtrain, xtest"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________________________________________________________________________\n",
            "Fold number: 1\n",
            "\n",
            "Epoch 1/10\n",
            "7620/7620 [==============================] - 215s 28ms/step - loss: 0.5397 - sparse_categorical_accuracy: 0.8506 - val_loss: 0.2132 - val_sparse_categorical_accuracy: 0.9423\n",
            "Epoch 2/10\n",
            "7620/7620 [==============================] - 215s 28ms/step - loss: 0.1462 - sparse_categorical_accuracy: 0.9604 - val_loss: 0.1542 - val_sparse_categorical_accuracy: 0.9600\n",
            "Epoch 3/10\n",
            "7620/7620 [==============================] - 214s 28ms/step - loss: 0.1000 - sparse_categorical_accuracy: 0.9728 - val_loss: 0.1077 - val_sparse_categorical_accuracy: 0.9718\n",
            "Epoch 4/10\n",
            "7620/7620 [==============================] - 215s 28ms/step - loss: 0.0744 - sparse_categorical_accuracy: 0.9790 - val_loss: 0.1165 - val_sparse_categorical_accuracy: 0.9725\n",
            "Epoch 5/10\n",
            "7620/7620 [==============================] - 215s 28ms/step - loss: 0.0593 - sparse_categorical_accuracy: 0.9830 - val_loss: 0.1072 - val_sparse_categorical_accuracy: 0.9747\n",
            "Epoch 6/10\n",
            "7620/7620 [==============================] - 214s 28ms/step - loss: 0.0486 - sparse_categorical_accuracy: 0.9861 - val_loss: 0.1104 - val_sparse_categorical_accuracy: 0.9740\n",
            "Epoch 7/10\n",
            "7620/7620 [==============================] - 215s 28ms/step - loss: 0.0416 - sparse_categorical_accuracy: 0.9880 - val_loss: 0.1115 - val_sparse_categorical_accuracy: 0.9761\n",
            "Epoch 00007: early stopping\n",
            "847/847 [==============================] - 7s 9ms/step - loss: 0.1115 - sparse_categorical_accuracy: 0.9761\n",
            "_________________________________________________________________________________________________________________________________\n",
            "Test Accuracy:\n",
            "0.9760814905166626\n",
            "_________________________________________________________________________________________________________________________________\n",
            "Test Loss:\n",
            "0.11145315319299698\n",
            "_________________________________________________________________________________________________________________________________\n",
            "Final balanced accuracy:\n",
            "0.9733667120640996\n",
            "_________________________________________________________________________________________________________________________________\n",
            "*********************************************************************************************************************************\n",
            "\n",
            "\n",
            "_________________________________________________________________________________________________________________________________\n",
            "Fold number: 2\n",
            "\n",
            "Epoch 1/10\n",
            "7620/7620 [==============================] - 214s 28ms/step - loss: 0.5431 - sparse_categorical_accuracy: 0.8503 - val_loss: 0.2111 - val_sparse_categorical_accuracy: 0.9425\n",
            "Epoch 2/10\n",
            "7620/7620 [==============================] - 215s 28ms/step - loss: 0.1469 - sparse_categorical_accuracy: 0.9608 - val_loss: 0.1433 - val_sparse_categorical_accuracy: 0.9625\n",
            "Epoch 3/10\n",
            "7620/7620 [==============================] - 214s 28ms/step - loss: 0.1006 - sparse_categorical_accuracy: 0.9725 - val_loss: 0.1141 - val_sparse_categorical_accuracy: 0.9702\n",
            "Epoch 4/10\n",
            "7620/7620 [==============================] - 214s 28ms/step - loss: 0.0755 - sparse_categorical_accuracy: 0.9789 - val_loss: 0.1036 - val_sparse_categorical_accuracy: 0.9720\n",
            "Epoch 5/10\n",
            "7620/7620 [==============================] - 214s 28ms/step - loss: 0.0593 - sparse_categorical_accuracy: 0.9831 - val_loss: 0.1115 - val_sparse_categorical_accuracy: 0.9726\n",
            "Epoch 6/10\n",
            "7620/7620 [==============================] - 214s 28ms/step - loss: 0.0478 - sparse_categorical_accuracy: 0.9863 - val_loss: 0.0948 - val_sparse_categorical_accuracy: 0.9767\n",
            "Epoch 7/10\n",
            "7620/7620 [==============================] - 214s 28ms/step - loss: 0.0415 - sparse_categorical_accuracy: 0.9881 - val_loss: 0.1068 - val_sparse_categorical_accuracy: 0.9748\n",
            "Epoch 8/10\n",
            "7620/7620 [==============================] - 214s 28ms/step - loss: 0.0359 - sparse_categorical_accuracy: 0.9897 - val_loss: 0.1191 - val_sparse_categorical_accuracy: 0.9739\n",
            "Epoch 00008: early stopping\n",
            "847/847 [==============================] - 8s 9ms/step - loss: 0.1191 - sparse_categorical_accuracy: 0.9739\n",
            "_________________________________________________________________________________________________________________________________\n",
            "Test Accuracy:\n",
            "0.9739406704902649\n",
            "_________________________________________________________________________________________________________________________________\n",
            "Test Loss:\n",
            "0.11910578608512878\n",
            "_________________________________________________________________________________________________________________________________\n",
            "Final balanced accuracy:\n",
            "0.9715321605237515\n",
            "_________________________________________________________________________________________________________________________________\n",
            "*********************************************************************************************************************************\n",
            "\n",
            "\n",
            "_________________________________________________________________________________________________________________________________\n",
            "Fold number: 3\n",
            "\n",
            "Epoch 1/10\n",
            "7620/7620 [==============================] - 215s 28ms/step - loss: 0.5312 - sparse_categorical_accuracy: 0.8529 - val_loss: 0.1894 - val_sparse_categorical_accuracy: 0.9464\n",
            "Epoch 2/10\n",
            "7620/7620 [==============================] - 215s 28ms/step - loss: 0.1462 - sparse_categorical_accuracy: 0.9604 - val_loss: 0.1248 - val_sparse_categorical_accuracy: 0.9668\n",
            "Epoch 3/10\n",
            "7620/7620 [==============================] - 215s 28ms/step - loss: 0.1002 - sparse_categorical_accuracy: 0.9729 - val_loss: 0.1167 - val_sparse_categorical_accuracy: 0.9694\n",
            "Epoch 4/10\n",
            "7620/7620 [==============================] - 215s 28ms/step - loss: 0.0741 - sparse_categorical_accuracy: 0.9789 - val_loss: 0.1009 - val_sparse_categorical_accuracy: 0.9753\n",
            "Epoch 5/10\n",
            "7620/7620 [==============================] - 215s 28ms/step - loss: 0.0592 - sparse_categorical_accuracy: 0.9831 - val_loss: 0.1181 - val_sparse_categorical_accuracy: 0.9711\n",
            "Epoch 6/10\n",
            "7620/7620 [==============================] - 215s 28ms/step - loss: 0.0488 - sparse_categorical_accuracy: 0.9862 - val_loss: 0.1023 - val_sparse_categorical_accuracy: 0.9765\n",
            "Epoch 00006: early stopping\n",
            "847/847 [==============================] - 7s 9ms/step - loss: 0.1023 - sparse_categorical_accuracy: 0.9765\n",
            "_________________________________________________________________________________________________________________________________\n",
            "Test Accuracy:\n",
            "0.9765235781669617\n",
            "_________________________________________________________________________________________________________________________________\n",
            "Test Loss:\n",
            "0.10233599692583084\n",
            "_________________________________________________________________________________________________________________________________\n",
            "Final balanced accuracy:\n",
            "0.9725513017660404\n",
            "_________________________________________________________________________________________________________________________________\n",
            "*********************************************************************************************************************************\n",
            "\n",
            "\n",
            "_________________________________________________________________________________________________________________________________\n",
            "Fold number: 4\n",
            "\n",
            "Epoch 1/10\n",
            "   1/7620 [..............................] - ETA: 0s - loss: 3.8919 - sparse_categorical_accuracy: 0.0000e+00WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0106s vs `on_train_batch_end` time: 0.0160s). Check your callbacks.\n",
            "7620/7620 [==============================] - 215s 28ms/step - loss: 0.5943 - sparse_categorical_accuracy: 0.8376 - val_loss: 0.1786 - val_sparse_categorical_accuracy: 0.9522\n",
            "Epoch 2/10\n",
            "7620/7620 [==============================] - 214s 28ms/step - loss: 0.1459 - sparse_categorical_accuracy: 0.9605 - val_loss: 0.1471 - val_sparse_categorical_accuracy: 0.9609\n",
            "Epoch 3/10\n",
            "7620/7620 [==============================] - 215s 28ms/step - loss: 0.0994 - sparse_categorical_accuracy: 0.9729 - val_loss: 0.1329 - val_sparse_categorical_accuracy: 0.9662\n",
            "Epoch 4/10\n",
            "7620/7620 [==============================] - 213s 28ms/step - loss: 0.0746 - sparse_categorical_accuracy: 0.9791 - val_loss: 0.1101 - val_sparse_categorical_accuracy: 0.9732\n",
            "Epoch 5/10\n",
            "7620/7620 [==============================] - 213s 28ms/step - loss: 0.0590 - sparse_categorical_accuracy: 0.9833 - val_loss: 0.1089 - val_sparse_categorical_accuracy: 0.9728\n",
            "Epoch 6/10\n",
            "7620/7620 [==============================] - 213s 28ms/step - loss: 0.0481 - sparse_categorical_accuracy: 0.9861 - val_loss: 0.1088 - val_sparse_categorical_accuracy: 0.9756\n",
            "Epoch 7/10\n",
            "7620/7620 [==============================] - 213s 28ms/step - loss: 0.0421 - sparse_categorical_accuracy: 0.9879 - val_loss: 0.1115 - val_sparse_categorical_accuracy: 0.9733\n",
            "Epoch 8/10\n",
            "7620/7620 [==============================] - 213s 28ms/step - loss: 0.0371 - sparse_categorical_accuracy: 0.9890 - val_loss: 0.1089 - val_sparse_categorical_accuracy: 0.9749\n",
            "Epoch 00008: early stopping\n",
            "847/847 [==============================] - 8s 9ms/step - loss: 0.1089 - sparse_categorical_accuracy: 0.9749\n",
            "_________________________________________________________________________________________________________________________________\n",
            "Test Accuracy:\n",
            "0.974936306476593\n",
            "_________________________________________________________________________________________________________________________________\n",
            "Test Loss:\n",
            "0.10888320952653885\n",
            "_________________________________________________________________________________________________________________________________\n",
            "Final balanced accuracy:\n",
            "0.9732986042272932\n",
            "_________________________________________________________________________________________________________________________________\n",
            "*********************************************************************************************************************************\n",
            "\n",
            "\n",
            "_________________________________________________________________________________________________________________________________\n",
            "Fold number: 5\n",
            "\n",
            "Epoch 1/10\n",
            "7620/7620 [==============================] - 213s 28ms/step - loss: 0.5374 - sparse_categorical_accuracy: 0.8517 - val_loss: 0.1979 - val_sparse_categorical_accuracy: 0.9464\n",
            "Epoch 2/10\n",
            "7620/7620 [==============================] - 213s 28ms/step - loss: 0.1499 - sparse_categorical_accuracy: 0.9589 - val_loss: 0.1488 - val_sparse_categorical_accuracy: 0.9612\n",
            "Epoch 3/10\n",
            "7620/7620 [==============================] - 213s 28ms/step - loss: 0.1013 - sparse_categorical_accuracy: 0.9720 - val_loss: 0.1292 - val_sparse_categorical_accuracy: 0.9663\n",
            "Epoch 4/10\n",
            "7620/7620 [==============================] - 213s 28ms/step - loss: 0.0772 - sparse_categorical_accuracy: 0.9785 - val_loss: 0.1161 - val_sparse_categorical_accuracy: 0.9702\n",
            "Epoch 5/10\n",
            "7620/7620 [==============================] - 213s 28ms/step - loss: 0.0610 - sparse_categorical_accuracy: 0.9826 - val_loss: 0.1125 - val_sparse_categorical_accuracy: 0.9705\n",
            "Epoch 6/10\n",
            "7620/7620 [==============================] - 213s 28ms/step - loss: 0.0506 - sparse_categorical_accuracy: 0.9854 - val_loss: 0.1111 - val_sparse_categorical_accuracy: 0.9720\n",
            "Epoch 7/10\n",
            "7620/7620 [==============================] - 213s 28ms/step - loss: 0.0421 - sparse_categorical_accuracy: 0.9879 - val_loss: 0.1110 - val_sparse_categorical_accuracy: 0.9750\n",
            "Epoch 8/10\n",
            "7620/7620 [==============================] - 213s 28ms/step - loss: 0.0371 - sparse_categorical_accuracy: 0.9893 - val_loss: 0.1158 - val_sparse_categorical_accuracy: 0.9754\n",
            "Epoch 9/10\n",
            "7620/7620 [==============================] - 213s 28ms/step - loss: 0.0335 - sparse_categorical_accuracy: 0.9904 - val_loss: 0.1380 - val_sparse_categorical_accuracy: 0.9733\n",
            "Epoch 00009: early stopping\n",
            "847/847 [==============================] - 7s 9ms/step - loss: 0.1380 - sparse_categorical_accuracy: 0.9733\n",
            "_________________________________________________________________________________________________________________________________\n",
            "Test Accuracy:\n",
            "0.9733121991157532\n",
            "_________________________________________________________________________________________________________________________________\n",
            "Test Loss:\n",
            "0.13800747692584991\n",
            "_________________________________________________________________________________________________________________________________\n",
            "Final balanced accuracy:\n",
            "0.9711795721620778\n",
            "_________________________________________________________________________________________________________________________________\n",
            "*********************************************************************************************************************************\n",
            "\n",
            "\n",
            "_________________________________________________________________________________________________________________________________\n",
            "Fold number: 6\n",
            "\n",
            "Epoch 1/10\n",
            "7620/7620 [==============================] - 213s 28ms/step - loss: 0.5310 - sparse_categorical_accuracy: 0.8536 - val_loss: 0.1724 - val_sparse_categorical_accuracy: 0.9517\n",
            "Epoch 2/10\n",
            "7620/7620 [==============================] - 213s 28ms/step - loss: 0.1472 - sparse_categorical_accuracy: 0.9603 - val_loss: 0.1239 - val_sparse_categorical_accuracy: 0.9668\n",
            "Epoch 3/10\n",
            "7620/7620 [==============================] - 212s 28ms/step - loss: 0.1010 - sparse_categorical_accuracy: 0.9724 - val_loss: 0.1051 - val_sparse_categorical_accuracy: 0.9721\n",
            "Epoch 4/10\n",
            "7620/7620 [==============================] - 212s 28ms/step - loss: 0.0758 - sparse_categorical_accuracy: 0.9788 - val_loss: 0.1037 - val_sparse_categorical_accuracy: 0.9746\n",
            "Epoch 5/10\n",
            "7620/7620 [==============================] - 212s 28ms/step - loss: 0.0597 - sparse_categorical_accuracy: 0.9832 - val_loss: 0.1030 - val_sparse_categorical_accuracy: 0.9730\n",
            "Epoch 6/10\n",
            "7620/7620 [==============================] - 212s 28ms/step - loss: 0.0486 - sparse_categorical_accuracy: 0.9860 - val_loss: 0.1058 - val_sparse_categorical_accuracy: 0.9755\n",
            "Epoch 7/10\n",
            "7620/7620 [==============================] - 214s 28ms/step - loss: 0.0425 - sparse_categorical_accuracy: 0.9879 - val_loss: 0.1015 - val_sparse_categorical_accuracy: 0.9764\n",
            "Epoch 8/10\n",
            "7620/7620 [==============================] - 213s 28ms/step - loss: 0.0362 - sparse_categorical_accuracy: 0.9894 - val_loss: 0.1121 - val_sparse_categorical_accuracy: 0.9763\n",
            "Epoch 9/10\n",
            "7620/7620 [==============================] - 213s 28ms/step - loss: 0.0320 - sparse_categorical_accuracy: 0.9907 - val_loss: 0.0960 - val_sparse_categorical_accuracy: 0.9788\n",
            "Epoch 10/10\n",
            "7620/7620 [==============================] - 213s 28ms/step - loss: 0.0289 - sparse_categorical_accuracy: 0.9918 - val_loss: 0.1229 - val_sparse_categorical_accuracy: 0.9748\n",
            "847/847 [==============================] - 7s 9ms/step - loss: 0.1229 - sparse_categorical_accuracy: 0.9748\n",
            "_________________________________________________________________________________________________________________________________\n",
            "Test Accuracy:\n",
            "0.9747886657714844\n",
            "_________________________________________________________________________________________________________________________________\n",
            "Test Loss:\n",
            "0.12287868559360504\n",
            "_________________________________________________________________________________________________________________________________\n",
            "Final balanced accuracy:\n",
            "0.9738836934987607\n",
            "_________________________________________________________________________________________________________________________________\n",
            "*********************************************************************************************************************************\n",
            "\n",
            "\n",
            "_________________________________________________________________________________________________________________________________\n",
            "Fold number: 7\n",
            "\n",
            "Epoch 1/10\n",
            "7620/7620 [==============================] - 213s 28ms/step - loss: 0.5141 - sparse_categorical_accuracy: 0.8575 - val_loss: 0.1989 - val_sparse_categorical_accuracy: 0.9455\n",
            "Epoch 2/10\n",
            "7620/7620 [==============================] - 214s 28ms/step - loss: 0.1475 - sparse_categorical_accuracy: 0.9602 - val_loss: 0.1542 - val_sparse_categorical_accuracy: 0.9593\n",
            "Epoch 3/10\n",
            "7620/7620 [==============================] - 215s 28ms/step - loss: 0.1003 - sparse_categorical_accuracy: 0.9721 - val_loss: 0.1142 - val_sparse_categorical_accuracy: 0.9711\n",
            "Epoch 4/10\n",
            "7620/7620 [==============================] - 216s 28ms/step - loss: 0.0746 - sparse_categorical_accuracy: 0.9794 - val_loss: 0.1166 - val_sparse_categorical_accuracy: 0.9705\n",
            "Epoch 5/10\n",
            "7620/7620 [==============================] - 216s 28ms/step - loss: 0.0587 - sparse_categorical_accuracy: 0.9832 - val_loss: 0.1095 - val_sparse_categorical_accuracy: 0.9721\n",
            "Epoch 6/10\n",
            "7620/7620 [==============================] - 216s 28ms/step - loss: 0.0490 - sparse_categorical_accuracy: 0.9856 - val_loss: 0.1111 - val_sparse_categorical_accuracy: 0.9747\n",
            "Epoch 7/10\n",
            "7620/7620 [==============================] - 216s 28ms/step - loss: 0.0413 - sparse_categorical_accuracy: 0.9882 - val_loss: 0.1077 - val_sparse_categorical_accuracy: 0.9752\n",
            "Epoch 8/10\n",
            "7620/7620 [==============================] - 216s 28ms/step - loss: 0.0353 - sparse_categorical_accuracy: 0.9893 - val_loss: 0.1202 - val_sparse_categorical_accuracy: 0.9736\n",
            "Epoch 9/10\n",
            "7620/7620 [==============================] - 216s 28ms/step - loss: 0.0324 - sparse_categorical_accuracy: 0.9904 - val_loss: 0.1244 - val_sparse_categorical_accuracy: 0.9748\n",
            "Epoch 00009: early stopping\n",
            "847/847 [==============================] - 7s 9ms/step - loss: 0.1244 - sparse_categorical_accuracy: 0.9748\n",
            "_________________________________________________________________________________________________________________________________\n",
            "Test Accuracy:\n",
            "0.9748255610466003\n",
            "_________________________________________________________________________________________________________________________________\n",
            "Test Loss:\n",
            "0.12442599982023239\n",
            "_________________________________________________________________________________________________________________________________\n",
            "Final balanced accuracy:\n",
            "0.9712161762135609\n",
            "_________________________________________________________________________________________________________________________________\n",
            "*********************************************************************************************************************************\n",
            "\n",
            "\n",
            "_________________________________________________________________________________________________________________________________\n",
            "Fold number: 8\n",
            "\n",
            "Epoch 1/10\n",
            "7620/7620 [==============================] - 216s 28ms/step - loss: 0.5365 - sparse_categorical_accuracy: 0.8524 - val_loss: 0.2163 - val_sparse_categorical_accuracy: 0.9427\n",
            "Epoch 2/10\n",
            "7620/7620 [==============================] - 215s 28ms/step - loss: 0.1479 - sparse_categorical_accuracy: 0.9595 - val_loss: 0.1297 - val_sparse_categorical_accuracy: 0.9661\n",
            "Epoch 3/10\n",
            "7620/7620 [==============================] - 216s 28ms/step - loss: 0.0996 - sparse_categorical_accuracy: 0.9726 - val_loss: 0.1217 - val_sparse_categorical_accuracy: 0.9687\n",
            "Epoch 4/10\n",
            "7620/7620 [==============================] - 215s 28ms/step - loss: 0.0758 - sparse_categorical_accuracy: 0.9792 - val_loss: 0.1084 - val_sparse_categorical_accuracy: 0.9731\n",
            "Epoch 5/10\n",
            "7620/7620 [==============================] - 216s 28ms/step - loss: 0.0589 - sparse_categorical_accuracy: 0.9835 - val_loss: 0.1257 - val_sparse_categorical_accuracy: 0.9693\n",
            "Epoch 6/10\n",
            "7620/7620 [==============================] - 215s 28ms/step - loss: 0.0491 - sparse_categorical_accuracy: 0.9857 - val_loss: 0.1077 - val_sparse_categorical_accuracy: 0.9768\n",
            "Epoch 7/10\n",
            "7620/7620 [==============================] - 217s 28ms/step - loss: 0.0420 - sparse_categorical_accuracy: 0.9877 - val_loss: 0.1239 - val_sparse_categorical_accuracy: 0.9732\n",
            "Epoch 8/10\n",
            "7620/7620 [==============================] - 216s 28ms/step - loss: 0.0370 - sparse_categorical_accuracy: 0.9894 - val_loss: 0.1232 - val_sparse_categorical_accuracy: 0.9741\n",
            "Epoch 00008: early stopping\n",
            "847/847 [==============================] - 8s 9ms/step - loss: 0.1232 - sparse_categorical_accuracy: 0.9741\n",
            "_________________________________________________________________________________________________________________________________\n",
            "Test Accuracy:\n",
            "0.9741242527961731\n",
            "_________________________________________________________________________________________________________________________________\n",
            "Test Loss:\n",
            "0.12320949137210846\n",
            "_________________________________________________________________________________________________________________________________\n",
            "Final balanced accuracy:\n",
            "0.9719270893164335\n",
            "_________________________________________________________________________________________________________________________________\n",
            "*********************************************************************************************************************************\n",
            "\n",
            "\n",
            "_________________________________________________________________________________________________________________________________\n",
            "Fold number: 9\n",
            "\n",
            "Epoch 1/10\n",
            "7620/7620 [==============================] - 216s 28ms/step - loss: 0.5305 - sparse_categorical_accuracy: 0.8530 - val_loss: 0.1712 - val_sparse_categorical_accuracy: 0.9539\n",
            "Epoch 2/10\n",
            "7620/7620 [==============================] - 216s 28ms/step - loss: 0.1493 - sparse_categorical_accuracy: 0.9595 - val_loss: 0.1145 - val_sparse_categorical_accuracy: 0.9695\n",
            "Epoch 3/10\n",
            "7620/7620 [==============================] - 216s 28ms/step - loss: 0.1012 - sparse_categorical_accuracy: 0.9724 - val_loss: 0.1096 - val_sparse_categorical_accuracy: 0.9695\n",
            "Epoch 4/10\n",
            "7620/7620 [==============================] - 217s 28ms/step - loss: 0.0763 - sparse_categorical_accuracy: 0.9789 - val_loss: 0.1162 - val_sparse_categorical_accuracy: 0.9712\n",
            "Epoch 5/10\n",
            "7620/7620 [==============================] - 216s 28ms/step - loss: 0.0600 - sparse_categorical_accuracy: 0.9832 - val_loss: 0.0965 - val_sparse_categorical_accuracy: 0.9742\n",
            "Epoch 6/10\n",
            "7620/7620 [==============================] - 216s 28ms/step - loss: 0.0489 - sparse_categorical_accuracy: 0.9858 - val_loss: 0.0965 - val_sparse_categorical_accuracy: 0.9769\n",
            "Epoch 7/10\n",
            "7620/7620 [==============================] - 216s 28ms/step - loss: 0.0419 - sparse_categorical_accuracy: 0.9882 - val_loss: 0.0962 - val_sparse_categorical_accuracy: 0.9774\n",
            "Epoch 8/10\n",
            "7620/7620 [==============================] - 216s 28ms/step - loss: 0.0362 - sparse_categorical_accuracy: 0.9897 - val_loss: 0.1030 - val_sparse_categorical_accuracy: 0.9768\n",
            "Epoch 9/10\n",
            "7620/7620 [==============================] - 215s 28ms/step - loss: 0.0323 - sparse_categorical_accuracy: 0.9905 - val_loss: 0.0981 - val_sparse_categorical_accuracy: 0.9781\n",
            "Epoch 00009: early stopping\n",
            "847/847 [==============================] - 7s 9ms/step - loss: 0.0981 - sparse_categorical_accuracy: 0.9781\n",
            "_________________________________________________________________________________________________________________________________\n",
            "Test Accuracy:\n",
            "0.9780738949775696\n",
            "_________________________________________________________________________________________________________________________________\n",
            "Test Loss:\n",
            "0.0981191024184227\n",
            "_________________________________________________________________________________________________________________________________\n",
            "Final balanced accuracy:\n",
            "0.9753841899564364\n",
            "_________________________________________________________________________________________________________________________________\n",
            "*********************************************************************************************************************************\n",
            "\n",
            "\n",
            "_________________________________________________________________________________________________________________________________\n",
            "Fold number: 10\n",
            "\n",
            "Epoch 1/10\n",
            "7620/7620 [==============================] - 218s 29ms/step - loss: 0.5531 - sparse_categorical_accuracy: 0.8474 - val_loss: 0.1947 - val_sparse_categorical_accuracy: 0.9468\n",
            "Epoch 2/10\n",
            "7620/7620 [==============================] - 216s 28ms/step - loss: 0.1494 - sparse_categorical_accuracy: 0.9594 - val_loss: 0.1389 - val_sparse_categorical_accuracy: 0.9638\n",
            "Epoch 3/10\n",
            "7620/7620 [==============================] - 216s 28ms/step - loss: 0.1013 - sparse_categorical_accuracy: 0.9725 - val_loss: 0.1212 - val_sparse_categorical_accuracy: 0.9672\n",
            "Epoch 4/10\n",
            "7620/7620 [==============================] - 216s 28ms/step - loss: 0.0761 - sparse_categorical_accuracy: 0.9787 - val_loss: 0.1088 - val_sparse_categorical_accuracy: 0.9717\n",
            "Epoch 5/10\n",
            "7620/7620 [==============================] - 216s 28ms/step - loss: 0.0608 - sparse_categorical_accuracy: 0.9827 - val_loss: 0.1164 - val_sparse_categorical_accuracy: 0.9728\n",
            "Epoch 6/10\n",
            "7620/7620 [==============================] - 217s 28ms/step - loss: 0.0499 - sparse_categorical_accuracy: 0.9857 - val_loss: 0.1165 - val_sparse_categorical_accuracy: 0.9735\n",
            "Epoch 00006: early stopping\n",
            "847/847 [==============================] - 8s 9ms/step - loss: 0.1165 - sparse_categorical_accuracy: 0.9735\n",
            "_________________________________________________________________________________________________________________________________\n",
            "Test Accuracy:\n",
            "0.9735336303710938\n",
            "_________________________________________________________________________________________________________________________________\n",
            "Test Loss:\n",
            "0.11653514206409454\n",
            "_________________________________________________________________________________________________________________________________\n",
            "Final balanced accuracy:\n",
            "0.9700749320772387\n",
            "_________________________________________________________________________________________________________________________________\n",
            "*********************************************************************************************************************************\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RELlj5eh1tCD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 497
        },
        "outputId": "6155878c-4be4-41a7-986e-361497dd0b0f"
      },
      "source": [
        "print('------------------------------------------------------------------------')\n",
        "print('Score per fold')\n",
        "for i in range(0, len(balancedAccuracies)):\n",
        "  print('------------------------------------------------------------------------')\n",
        "  print(f'> Fold {i+1} - Loss: {losses[i]} - class-wise Accuracy: {balancedAccuracies[i]}% Accuracy - {baseAccuracies[i]} - Precision: {precisions[i]} - Recall: {recalls[i]} - F1: {f1scores[i]}')\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Average scores for all folds:')\n",
        "print(f'> Accuracy: {np.mean(balancedAccuracies)} (+- {np.std(balancedAccuracies)})')\n",
        "print(f'> Loss: {np.mean(losses)}')\n",
        "print('------------------------------------------------------------------------')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Score per fold\n",
            "------------------------------------------------------------------------\n",
            "> Fold 1 - Loss: 0.11145315319299698 - class-wise Accuracy: 0.9733667120640996% Accuracy - 0.9760814905166626 - Precision: 0.9762388450325744 - Recall: 0.9760815000738225 - F1: 0.9760372653784921\n",
            "------------------------------------------------------------------------\n",
            "> Fold 2 - Loss: 0.11910578608512878 - class-wise Accuracy: 0.9715321605237515% Accuracy - 0.9739406704902649 - Precision: 0.9742056417904237 - Recall: 0.9739406466853684 - F1: 0.9738678089594694\n",
            "------------------------------------------------------------------------\n",
            "> Fold 3 - Loss: 0.10233599692583084 - class-wise Accuracy: 0.9725513017660404% Accuracy - 0.9765235781669617 - Precision: 0.9766439748422074 - Recall: 0.9765235687128566 - F1: 0.9765030821539726\n",
            "------------------------------------------------------------------------\n",
            "> Fold 4 - Loss: 0.10888320952653885 - class-wise Accuracy: 0.9732986042272932% Accuracy - 0.974936306476593 - Precision: 0.9752946940308536 - Recall: 0.9749363257170278 - F1: 0.974989567035113\n",
            "------------------------------------------------------------------------\n",
            "> Fold 5 - Loss: 0.13800747692584991 - class-wise Accuracy: 0.9711795721620778% Accuracy - 0.9733121991157532 - Precision: 0.9736955732985628 - Recall: 0.973312170093389 - F1: 0.973337822381129\n",
            "------------------------------------------------------------------------\n",
            "> Fold 6 - Loss: 0.12287868559360504 - class-wise Accuracy: 0.9738836934987607% Accuracy - 0.9747886657714844 - Precision: 0.9749694906734145 - Recall: 0.9747886752057879 - F1: 0.9747816319780572\n",
            "------------------------------------------------------------------------\n",
            "> Fold 7 - Loss: 0.12442599982023239 - class-wise Accuracy: 0.9712161762135609% Accuracy - 0.9748255610466003 - Precision: 0.9751196730864289 - Recall: 0.9748255878335979 - F1: 0.9748586760498676\n",
            "------------------------------------------------------------------------\n",
            "> Fold 8 - Loss: 0.12320949137210846 - class-wise Accuracy: 0.9719270893164335% Accuracy - 0.9741242527961731 - Precision: 0.974448804174638 - Recall: 0.9741242479052084 - F1: 0.9741248427652238\n",
            "------------------------------------------------------------------------\n",
            "> Fold 9 - Loss: 0.0981191024184227 - class-wise Accuracy: 0.9753841899564364% Accuracy - 0.9780738949775696 - Precision: 0.9785036071653967 - Recall: 0.9780738990808756 - F1: 0.9781593270293797\n",
            "------------------------------------------------------------------------\n",
            "> Fold 10 - Loss: 0.11653514206409454 - class-wise Accuracy: 0.9700749320772387% Accuracy - 0.9735336303710938 - Precision: 0.9738162621590293 - Recall: 0.9735336458602488 - F1: 0.9735150421498385\n",
            "------------------------------------------------------------------------\n",
            "Average scores for all folds:\n",
            "> Accuracy: 0.9724414431805692 (+- 0.0014875512865219127)\n",
            "> Loss: 0.11649540439248085\n",
            "------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "uLlTRcMM_h0k"
      },
      "source": [
        "## Resource limits in Colab Pro\n",
        "\n",
        "Your resources are not unlimited in Colab Pro. To make the most of Colab Pro, please avoid using resources when you don't need them. For example, only use a GPU or high-RAM runtime when required, and close Colab tabs when finished.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "mm8FzEidvPs6"
      },
      "source": [
        "## Send us feedback!\n",
        "\n",
        "If you have any feedback for us, please let us know. The best way to send feedback is by using the Help > 'Send feedback...' menu. If you encounter usage limits in Colab Pro and would be interested in a product with higher usage limits, do let us know.\n",
        "\n",
        "If you encounter errors or other issues with billing (payments) for Colab Pro, please email colab-billing@google.com."
      ]
    }
  ]
}